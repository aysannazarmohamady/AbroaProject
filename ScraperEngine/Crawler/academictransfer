import requests
from bs4 import BeautifulSoup
import re
import time
import pandas as pd

# Initial settings
BASE_URL = 'https://www.academictransfer.com/en/jobs/?vacancy_type=scientific&q=&function_types=1&order=&page='
START_PAGE = 1
END_PAGE = 30  # Number of pages to scrape
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

def create_google_search_link(supervisor, university):
    query = f"{supervisor} {university}".replace(" ", "+")
    return f"https://www.google.com/search?q={query}"

def extract_links(google_search_link, domain):
    response = requests.get(google_search_link, headers=HEADERS)
    soup = BeautifulSoup(response.text, 'html.parser')
    links = []
    for div in soup.find_all('div', class_='MjjYud'):
        a_tag = div.find('a', jsname='UWckNb')
        if a_tag and 'href' in a_tag.attrs and domain in a_tag['href']:
            links.append(a_tag['href'])
    return links

def extract_job_details(job_url):
    job_response = requests.get(job_url)
    job_soup = BeautifulSoup(job_response.text, 'html.parser')

    funds = ""
    deadline = ""
    overview = ""
    emails = []
    institution_links = []
    supervisors = []
    additional_info = []

    # Extract funds
    specs_div = job_soup.find('div', class_='_Specs_Specs_IKUE-UWA')
    if specs_div:
        li_funds_tag = specs_div.find('li', class_='_Specs_salary_X-hy9Mqe')
        if li_funds_tag:
            span_tag_funds = li_funds_tag.find('span')
            if span_tag_funds:
                funds = span_tag_funds.text.strip()

    # Extract deadline
    meta_div = job_soup.find('div', class_='_Meta_meta_OzgOmmvA')
    if meta_div:
        td_deadline = meta_div.find('td', {'data-label': 'Deadline'})
        if td_deadline:
            deadline = td_deadline.text.strip()

    # Extract overview
    section_body_div = job_soup.find('div', class_='_Section_body_uVSjrQzu')
    if section_body_div:
        overview = section_body_div.text.strip().split("\n")[0]

    # Extract additional information from all sections
    section_divs = job_soup.find_all('div', class_='_Section_Section_a9UlTldI')
    for section_div in section_divs:
        text = section_div.get_text()

        # Extract emails and institution links
        for a_tag in section_div.find_all('a', href=True):
            href = a_tag['href']
            if href.startswith('mailto:'):
                emails.append(href.replace('mailto:', ''))
            elif href.startswith('https://') and 'maps.google.com' not in href and 'academictransfer.com' not in href:
                institution_links.append(href)

        # Extract supervisors
        supervisors.extend(re.findall(r'(Dr\.|dr\.|Prof\.|Professor)\s([A-Z][a-z]+(?:\s[A-Z][a-z]+)+)', text))

        # Extract emails from text
        emails.extend(re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', text))

        # Add additional information
        sentences = re.split(r'(?<=[.!?])\s+', text)
        additional_info.extend([s.strip() for s in sentences if 'contact' in s.lower() or '@' in s or any(title in s for title in ['Dr.', 'dr.', 'Prof.', 'Professor'])])

    # Remove duplicates
    emails = list(set(emails))
    institution_links = list(set(institution_links))
    supervisors = list(set([s[1] for s in supervisors]))
    additional_info = list(set(additional_info))

    return funds, deadline, overview, emails, institution_links, supervisors, additional_info

# List to store job data
data = []

for page in range(START_PAGE, END_PAGE + 1):
    url = f"{BASE_URL}{page}"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    for div in soup.find_all('div', class_='_Vacancy_Vacancy_7P-yunLb'):
        h6_tag = div.find('h6')
        if not h6_tag:
            continue

        span_tag = h6_tag.find('span')
        if not span_tag:
            continue

        university_div = div.find('div', class_='_Employer_Employer_YBmukMHl')
        if not university_div:
            continue

        university = university_div.text.strip()
        a_tag = div.find('a')
        if not a_tag or 'href' not in a_tag.attrs:
            continue

        job_url = 'https://www.academictransfer.com' + a_tag['href']

        funds, deadline, overview, emails, institution_links, supervisors, additional_info = extract_job_details(job_url)

        extra_info = []
        if deadline:
            extra_info.append(f"Deadline: {deadline}")
        extra_info.extend(additional_info)

        job_data = {
            'title': span_tag.text,
            'country': 'Netherlands',
            'university': university,
            'url': job_url,
            'funds': funds or None,
            'level': "PhD",
            'extra': '; '.join(extra_info) if extra_info else None,
            'overview': overview or None,
            'email': ', '.join(emails) if emails else None,
            'institution_link': ', '.join(institution_links) if institution_links else None,
            'supervisor': ', '.join(supervisors) if supervisors else None,
            'google_link': None,
            'researchgate_link': None,
            'scholar_link': None,
            'linkedin_link': None
        }

        if supervisors:
            google_links = []
            researchgate_links = []
            scholar_links = []
            linkedin_links = []
            for supervisor in supervisors:
                google_search_link = create_google_search_link(supervisor, university)
                google_links.append(google_search_link)

                researchgate_links.extend(extract_links(google_search_link, 'https://www.researchgate.net/'))
                scholar_links.extend(extract_links(google_search_link, 'https://scholar.google.com/'))
                linkedin_links.extend(extract_links(google_search_link, 'https://www.linkedin.com/'))

            job_data['google_link'] = ', '.join(google_links)
            job_data['researchgate_link'] = ', '.join(researchgate_links) if researchgate_links else None
            job_data['scholar_link'] = ', '.join(scholar_links) if scholar_links else None
            job_data['linkedin_link'] = ', '.join(linkedin_links) if linkedin_links else None

        data.append(job_data)

        time.sleep(2)  # Delay to prevent too frequent requests

# Create DataFrame
df = pd.DataFrame(data)

# Display the first few rows of the DataFrame
print(df.head())

# Save DataFrame to CSV file
df.to_csv('academic_transfer_jobs.csv', index=False)
